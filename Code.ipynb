{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d147bcf5",
   "metadata": {},
   "source": [
    "1. Tahap Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin, urlparse, urlencode\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "DEFAULT_TZ = \"Asia/Jakarta\"\n",
    "INDEX_BASE = \"https://indeks.kompas.com\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"),\n",
    "    \"Accept-Language\": \"id,en-US;q=0.9,en;q=0.8\",\n",
    "    \"Referer\": \"https://indeks.kompas.com/\"\n",
    "}\n",
    "\n",
    "URL_DATE_RE = re.compile(r\"/read/(\\d{4})/(\\d{2})/(\\d{2})/(\\d{6,9})(?:/|$)\")\n",
    "\n",
    "DISASTER_KEYWORDS = [\n",
    "    \"gempa\", \"banjir\", \"longsor\", \"tsunami\", \"angin puting beliung\", \"angin kencang\",\n",
    "    \"cuaca ekstrem\", \"erupsi\", \"letusan\", \"gunung\", \"kebakaran hutan\", \"karhutla\",\n",
    "    \"kekeringan\", \"badai\", \"topan\", \"gelombang tinggi\", \"abrasi\", \"lahar\", \"banjir bandang\",\n",
    "    \"bmkg\", \"pvmbg\", \"bnpb\", \"basarnas\",\n",
    "]\n",
    "\n",
    "def make_session():\n",
    "    session = requests.Session()\n",
    "    retry = Retry(total=3, backoff_factor=0.7,\n",
    "                  status_forcelist=[429, 500, 502, 503, 504],\n",
    "                  allowed_methods=[\"GET\"])\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def get(session, url, timeout=25):\n",
    "    r = session.get(url, headers=HEADERS, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def dt_from_url(tz, link):\n",
    "    try:\n",
    "        m = URL_DATE_RE.search(urlparse(link).path)\n",
    "        if not m:\n",
    "            return None\n",
    "        y, mo, d, hms = m.groups()\n",
    "        hh, mm, ss = int(hms[:2]), int(hms[2:4]), int(hms[4:6])\n",
    "        return tz.localize(datetime(int(y), int(mo), int(d), hh, mm, ss))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def in_range(start_dt, end_dt, dt):\n",
    "    return (dt is not None) and (start_dt <= dt <= end_dt)\n",
    "\n",
    "def collect_links_for_date(session, tz, day_dt, max_pages, sleep_sec, timeout):\n",
    "    y, m, d = day_dt.year, day_dt.month, day_dt.day\n",
    "    daily_links = set()\n",
    "    pattern = re.compile(fr\"/read/{y}/{m:02d}/{d:02d}/\\d{{6,9}}/\")\n",
    "\n",
    "    site = \"nasional\"\n",
    "    for page in range(1, max_pages + 1):\n",
    "        qs = urlencode({\"site\": site, \"date\": f\"{y}-{m:02d}-{d:02d}\", \"page\": page})\n",
    "        url = f\"{INDEX_BASE}/?{qs}\"\n",
    "\n",
    "        try:\n",
    "            time.sleep(sleep_sec)\n",
    "            resp = get(session, url, timeout=timeout)\n",
    "        except Exception as ex:\n",
    "            print(f\"[INDEKS] Gagal {url} -> {ex}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        anchors = soup.find_all(\"a\", href=True)\n",
    "\n",
    "        found = 0\n",
    "        for a in anchors:\n",
    "            href = a[\"href\"].split(\"?\")[0]\n",
    "            if href.startswith(\"//\"):\n",
    "                href = \"https:\" + href\n",
    "            if href.startswith(\"/\"):\n",
    "                href = urljoin(\"https://www.kompas.com\", href)\n",
    "\n",
    "            try:\n",
    "                p = urlparse(href)\n",
    "                if (\"kompas.com\" in p.netloc) and pattern.search(p.path):\n",
    "                    daily_links.add(href)\n",
    "                    found += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"[INDEKS] nasional {y}-{m:02d}-{d:02d} page={page} -> {found} link\")\n",
    "        if found == 0:\n",
    "            break\n",
    "\n",
    "    return daily_links\n",
    "\n",
    "def extract_article_detail(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    title = None\n",
    "    for sel in [\"h1.read__title\", \"h1.detail__title\", \"h1.headline__title\", \"h1\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el and el.get_text(strip=True):\n",
    "            title = el.get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    author = None\n",
    "    for sel in [\".read__author\", \".author\", '[rel=\"author\"]', \".byline\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el and el.get_text(strip=True):\n",
    "            author = el.get_text(\" \", strip=True)\n",
    "            break\n",
    "\n",
    "    section = None\n",
    "    for sel in ['meta[property=\"article:section\"]', 'meta[name=\"section\"]']:\n",
    "        m = soup.select_one(sel)\n",
    "        if m and m.get(\"content\"):\n",
    "            section = m.get(\"content\").strip()\n",
    "            break\n",
    "\n",
    "    if not section:\n",
    "        crumbs = [a.get_text(\" \", strip=True)\n",
    "                  for a in soup.select(\".breadcrumb a, .breadcrumb__link, a.breadcrumb__link\")\n",
    "                  if a.get_text(strip=True)]\n",
    "        crumbs = [c for c in crumbs if c.lower() not in (\"kompas.com\",)]\n",
    "        if crumbs:\n",
    "            section = crumbs[-1]\n",
    "    if not section:\n",
    "        section = \"Nasional\"\n",
    "\n",
    "    summary = \"\"\n",
    "    for sel in [\".read__content p\", \".article__content p\", \"article p\"]:\n",
    "        nodes = soup.select(sel)\n",
    "        if nodes:\n",
    "            texts = [p.get_text(\" \", strip=True) for p in nodes if p.get_text(strip=True)]\n",
    "            if texts:\n",
    "                summary = \" \".join(texts)[:5000]\n",
    "                break\n",
    "\n",
    "    return (title or \"\").strip(), (author or \"\").strip(), section, summary\n",
    "\n",
    "def is_disaster_row(title, summary):\n",
    "    txt = f\"{title or ''} {summary or ''}\".lower()\n",
    "    return any(kw in txt for kw in DISASTER_KEYWORDS)\n",
    "\n",
    "def run(args):\n",
    "    tz = pytz.timezone(args.timezone)\n",
    "    start_dt = tz.localize(datetime.strptime(args.start, \"%Y-%m-%d\"))\n",
    "    end_dt = tz.localize(datetime.strptime(args.end, \"%Y-%m-%d\")) + timedelta(hours=23, minutes=59, seconds=59)\n",
    "\n",
    "    print(f\"\\n=== Scraping NASIONAL bencana alam {args.start} s/d {args.end} ===\")\n",
    "\n",
    "    session = make_session()\n",
    "\n",
    "    all_links = set()\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        daily = collect_links_for_date(session, tz, cur, args.max_pages, args.sleep, args.timeout)\n",
    "        all_links |= daily\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "    print(f\"Total link unik: {len(all_links)}\")\n",
    "\n",
    "    links_in_range = []\n",
    "    for u in sorted(all_links):\n",
    "        dt_url = dt_from_url(tz, u)\n",
    "        if in_range(start_dt, end_dt, dt_url):\n",
    "            links_in_range.append((u, dt_url))\n",
    "    print(f\"Link dalam rentang tanggal: {len(links_in_range)}\")\n",
    "\n",
    "    rows = []\n",
    "    for i, (link, dt_url) in enumerate(links_in_range, 1):\n",
    "        try:\n",
    "            time.sleep(args.sleep)\n",
    "            r = get(session, link, timeout=args.timeout)\n",
    "            title, author, section, summary = extract_article_detail(r.text)\n",
    "        except Exception as ex:\n",
    "            print(f\"[ERR] {link} -> {ex}\")\n",
    "            title = author = summary = \"\"\n",
    "            section = \"Nasional\"\n",
    "\n",
    "        if is_disaster_row(title, summary):\n",
    "            rows.append({\n",
    "                \"title\": title,\n",
    "                \"url\": link,\n",
    "                \"author\": author,\n",
    "                \"section\": section,\n",
    "                \"published_local\": dt_url.strftime(\"%Y-%m-%d %H:%M:%S %Z\"),\n",
    "                \"summary\": summary\n",
    "            })\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(f\"[PROGRESS] {i}/{len(links_in_range)} (disaster: {len(rows)})\")\n",
    "\n",
    "    if not rows:\n",
    "        print(\"TIDAK ada artikel bencana di rentang waktu ini.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "\n",
    "    df[\"published_dt\"] = pd.to_datetime(df[\"published_local\"].str.replace(\" WIB\",\"\", regex=False), errors=\"coerce\")\n",
    "    df[\"bulan\"] = df[\"published_dt\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================\n",
    "#  SCRAPE JAN–JUN 2025\n",
    "# ======================\n",
    "args = SimpleNamespace(\n",
    "    start=\"2025-01-01\",\n",
    "    end=\"2025-06-30\",\n",
    "    timezone=\"Asia/Jakarta\",\n",
    "    sleep=1.5,\n",
    "    timeout=25,\n",
    "    max_pages=6\n",
    ")\n",
    "df_janjun = run(args)\n",
    "\n",
    "# ======================\n",
    "#  SCRAPE JUL–SEP 2025\n",
    "# ======================\n",
    "args = SimpleNamespace(\n",
    "    start=\"2025-07-01\",\n",
    "    end=\"2025-09-30\",\n",
    "    timezone=\"Asia/Jakarta\",\n",
    "    sleep=1.5,\n",
    "    timeout=25,\n",
    "    max_pages=6\n",
    ")\n",
    "df_julsep = run(args)\n",
    "\n",
    "# ======================\n",
    "#  GABUNG & SIMPAN EXCEL\n",
    "# ======================\n",
    "df_total = pd.concat([df_janjun, df_julsep], ignore_index=True)\n",
    "df_total = df_total.drop_duplicates(subset=\"url\").reset_index(drop=True)\n",
    "\n",
    "output_path = \"kompas_bencana_nasional_2025.xlsx\"\n",
    "df_total.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"\\n=== FILE BERHASIL DISIMPAN ===\")\n",
    "print(\" ->\", output_path)\n",
    "print(\"Total artikel:\", len(df_total))\n",
    "\n",
    "df_total.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be938138",
   "metadata": {},
   "source": [
    "2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9e7c3",
   "metadata": {},
   "source": [
    "Import & Konfigurasi Dasar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b96676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from html import unescape\n",
    "import pandas as pd\n",
    "\n",
    "# Apakah mau menghapus emoji/symbol\n",
    "REMOVE_EMOJI = True\n",
    "\n",
    "# Normalisasi singkatan\n",
    "NORMALIZE_MAP = {\n",
    "    r\"\\bKab\\.?\\b\": \"Kabupaten\",\n",
    "    r\"\\bKec\\.?\\b\": \"Kecamatan\",\n",
    "    r\"\\bKota\\.?\\b\": \"Kota\",\n",
    "    r\"\\bProv\\.?\\b\": \"Provinsi\",\n",
    "    r\"\\bWali Kota\\b\": \"Walikota\",\n",
    "    r\"\\bWakil Presiden\\b\": \"Wakil Presiden\",\n",
    "    r\"\\bDr\\.\\b\": \"Dr.\",\n",
    "    r\"\\bPres\\.*\\b\": \"Presiden\"\n",
    "}\n",
    "\n",
    "# Regex Helper\n",
    "TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "MULTI_SPACE_RE = re.compile(r\"\\s{2,}\")\n",
    "REPEAT_PUNCT_RE = re.compile(r\"([!?.,])\\1{2,}\")\n",
    "CONTROL_CHARS = re.compile(r\"[\\r\\n\\t]+\")\n",
    "SPACE_PUNCT_RE = re.compile(r\"\\s+([,.;:!?])\")\n",
    "LEADING_TRAILING_SPACE_RE = re.compile(r\"^\\s+|\\s+$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9f789",
   "metadata": {},
   "source": [
    "Fungsi Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e577b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_and_symbols(text: str) -> str:\n",
    "    normalized = unicodedata.normalize(\"NFKC\", text)\n",
    "    out_chars = []\n",
    "    for ch in normalized:\n",
    "        cat = unicodedata.category(ch)\n",
    "        if cat.startswith((\"C\", \"S\")):\n",
    "            continue\n",
    "        out_chars.append(ch)\n",
    "    return \"\".join(out_chars)\n",
    "\n",
    "\n",
    "def detect_text_column(df: pd.DataFrame):\n",
    "    candidates = [\"cleaned_text\", \"clean_text\", \"text\", \"summary\", \"body\",\n",
    "                  \"isi\", \"berita\", \"content\", \"headline\"]\n",
    "    for cand in candidates:\n",
    "        for col in df.columns:\n",
    "            if cand.lower() == col.lower():\n",
    "                return col\n",
    "\n",
    "    # Jika tidak menemukan, pilih kolom object terpanjang\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    if not obj_cols:\n",
    "        return df.columns[0]\n",
    "\n",
    "    best = max(\n",
    "        obj_cols,\n",
    "        key=lambda c: df[c].dropna().astype(str).map(len).mean()\n",
    "        if len(df[c].dropna()) > 0 else 0\n",
    "    )\n",
    "    return best\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = unescape(s)\n",
    "    s = s.lstrip(\"\\ufeff\")\n",
    "    s = TAG_RE.sub(\" \", s)\n",
    "    s = URL_RE.sub(\" \", s)\n",
    "    s = CONTROL_CHARS.sub(\" \", s)\n",
    "\n",
    "    if REMOVE_EMOJI:\n",
    "        s = remove_emoji_and_symbols(s)\n",
    "\n",
    "    s = REPEAT_PUNCT_RE.sub(r\"\\1\", s)\n",
    "\n",
    "    for pat, rep in NORMALIZE_MAP.items():\n",
    "        s = re.sub(pat, rep, s, flags=re.IGNORECASE)\n",
    "\n",
    "    s = MULTI_SPACE_RE.sub(\" \", s)\n",
    "    s = SPACE_PUNCT_RE.sub(r\"\\1\", s)\n",
    "    s = LEADING_TRAILING_SPACE_RE.sub(\"\", s)\n",
    "    return s\n",
    "\n",
    "def run_preprocessing(input_path, out_dir):\n",
    "    input_path = Path(input_path)\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"File tidak ditemukan: {input_path}\")\n",
    "\n",
    "    # Load Excel/CSV\n",
    "    if input_path.suffix.lower() in [\".xls\", \".xlsx\"]:\n",
    "        xls = pd.ExcelFile(input_path)\n",
    "        df = pd.read_excel(input_path, sheet_name=xls.sheet_names[0])\n",
    "    else:\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "    print(f\"Loaded file: {input_path} | shape = {df.shape}\")\n",
    "\n",
    "    text_col = detect_text_column(df)\n",
    "    print(f\"Detected text column: {text_col}\")\n",
    "\n",
    "    df[\"original_text\"] = df[text_col].astype(str).fillna(\"\")\n",
    "    df[\"cleaned_text\"] = df[\"original_text\"].apply(clean_text)\n",
    "    df[\"cleaned_text_preview\"] = df[\"cleaned_text\"].str.slice(0, 200)\n",
    "\n",
    "    # Create output folder\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_csv = out_dir / (input_path.stem + \"_preprocessed.csv\")\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Preprocessed CSV saved to: {out_csv}\")\n",
    "\n",
    "    print(\"\\nRingkasan:\")\n",
    "    print(f\" - total rows: {len(df)}\")\n",
    "    print(f\" - original_text kosong: {(df['original_text'].str.strip() == '').sum()}\")\n",
    "    print(f\" - cleaned_text kosong: {(df['cleaned_text'].str.strip() == '').sum()}\")\n",
    "\n",
    "    print(\"\\nContoh 10 baris (original → cleaned):\\n\")\n",
    "    for i, row in df[['original_text', 'cleaned_text_preview']].head(10).iterrows():\n",
    "        print(f\"[{i}] ORIGINAL: {row['original_text'][:200]!r}\")\n",
    "        print(f\"     CLEANED: {row['cleaned_text_preview']!r}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d79ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: D:\\SEMESTER 7\\Equivalensi jurnal P Setio\\coding\\kompas_bencana_nasional_2025.xlsx | shape = (1429, 6)\n",
      "Detected text column: summary\n",
      "Preprocessed CSV saved to: D:\\SEMESTER 7\\Equivalensi jurnal P Setio\\coding\\kompas_bencana_nasional_2025_preprocessed.csv\n",
      "\n",
      "Ringkasan:\n",
      " - total rows: 1429\n",
      " - original_text kosong: 0\n",
      " - cleaned_text kosong: 0\n",
      "\n",
      "Contoh 10 baris (original → cleaned):\n",
      "\n",
      "[0] ORIGINAL: 'JAKARTA, KOMPAS.com - Presiden Prabowo Subianto mendoakan agar Indonesia diberikan kesejahteraan, perdamaian, dan kebaikan di tahun 2025. Harapan dan doa Prabowo tersebut disampaikan oleh Menteri Koor'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Presiden Prabowo Subianto mendoakan agar Indonesia diberikan kesejahteraan, perdamaian, dan kebaikan di tahun 2025. Harapan dan doa Prabowo tersebut disampaikan oleh Menteri Koor'\n",
      "------------------------------------------------------------\n",
      "[1] ORIGINAL: 'JAKARTA, KOMPAS.com - Satgas Operasi Damai Cartenz 2024 mencatatkan sejumlah capaian selama menggelar operasi di Papua sepanjang tahun 2024. Capaian tersebut, di antaranya, penegakan hukum terhadap Ke'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Satgas Operasi Damai Cartenz 2024 mencatatkan sejumlah capaian selama menggelar operasi di Papua sepanjang tahun 2024. Capaian tersebut, di antaranya, penegakan hukum terhadap Ke'\n",
      "------------------------------------------------------------\n",
      "[2] ORIGINAL: 'JAKARTA, KOMPAS.com - Presiden Prabowo Subianto menyapa ribuan prajurit TNI yang ditugaskan di wilayah Papua. Momen itu terjadi saat Prabowo melakukan panggilan video bersama Menteri Pertahanan Sjafri'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Presiden Prabowo Subianto menyapa ribuan prajurit TNI yang ditugaskan di wilayah Papua. Momen itu terjadi saat Prabowo melakukan panggilan video bersama Menteri Pertahanan Sjafri'\n",
      "------------------------------------------------------------\n",
      "[3] ORIGINAL: 'JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor Jakarta Pusat menegur dan menyebut saksi di persidangan terkadang sudah dikondisikan untuk menyampaikan keterangan sesuai pesanan pihak tertentu. Peristi'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor Jakarta Pusat menegur dan menyebut saksi di persidangan terkadang sudah dikondisikan untuk menyampaikan keterangan sesuai pesanan pihak tertentu. Peristi'\n",
      "------------------------------------------------------------\n",
      "[4] ORIGINAL: 'JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhanuddin bersama Menko Polkam Budi Gunawan Menko Polkam, dan jajaran menteri lain menggelar rapat kordinasi Desk Koordinasi Pencegahan Korupsi dan Perbaikan '\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhanuddin bersama Menko Polkam Budi Gunawan Menko Polkam, dan jajaran menteri lain menggelar rapat kordinasi Desk Koordinasi Pencegahan Korupsi dan Perbaikan '\n",
      "------------------------------------------------------------\n",
      "[5] ORIGINAL: 'JAKARTA, KOMPAS.com - Mantan pejabat Badan Nasional Pencarian dan Pertolongan (Basarnas), Kamil menyebut, pejabat Eselon III di tempatnya bekerja mendapat tunjangan hari raya (THR) Rp 10 juta yang ber'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Mantan pejabat Badan Nasional Pencarian dan Pertolongan (Basarnas), Kamil menyebut, pejabat Eselon III di tempatnya bekerja mendapat tunjangan hari raya (THR) Rp 10 juta yang ber'\n",
      "------------------------------------------------------------\n",
      "[6] ORIGINAL: 'JAKARTA, KOMPAS.com - Mantan pejabat Badan Nasional Pencarian dan Pertolongan (Basarnas), Kamil mengaku meminjamkan uang kepada pengusaha pemenang tender di Basarnas, William Widarta, senilai miliaran'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Mantan pejabat Badan Nasional Pencarian dan Pertolongan (Basarnas), Kamil mengaku meminjamkan uang kepada pengusaha pemenang tender di Basarnas, William Widarta, senilai miliaran'\n",
      "------------------------------------------------------------\n",
      "[7] ORIGINAL: 'JAKARTA, KOMPAS.com - Mantan pejabat Badan Nasional Pencarian dan Pertolongan (Basarnas), Kamil, mengaku diminta menjadi kurir untuk mengantarkan uang kepada Kepala Auditorat 1D pada Badan Pemeriksa K'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Mantan pejabat Badan Nasional Pencarian dan Pertolongan (Basarnas), Kamil, mengaku diminta menjadi kurir untuk mengantarkan uang kepada Kepala Auditorat 1D pada Badan Pemeriksa K'\n",
      "------------------------------------------------------------\n",
      "[8] ORIGINAL: 'JAKARTA, KOMPAS.com - Mantan Kepala Unit Layanan Pengadaan (ULP) Badan SAR Nasional (Basarnas), Kamil mengaku pernah diperintahkan mengantarkan uang untuk auditor Badan Pemeriksa Keuangan (BPK), Firma'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Mantan Kepala Unit Layanan Pengadaan (ULP) Badan SAR Nasional (Basarnas), Kamil mengaku pernah diperintahkan mengantarkan uang untuk auditor Badan Pemeriksa Keuangan (BPK), Firma'\n",
      "------------------------------------------------------------\n",
      "[9] ORIGINAL: 'JAKARTA, KOMPAS.com - Anggota Komisi IV DPR dari Fraksi PKB Daniel Johan mengkritisi wacana Menteri Kehutanan Raja Juli Antoni yang\\xa0\\xa0mengidentifikasi hutan untuk mendukung ketahanan pangan, energi, da'\n",
      "     CLEANED: 'JAKARTA, KOMPAS.com - Anggota Komisi IV DPR dari Fraksi PKB Daniel Johan mengkritisi wacana Menteri Kehutanan Raja Juli Antoni yang mengidentifikasi hutan untuk mendukung ketahanan pangan, energi, dan'\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>section</th>\n",
       "      <th>published_local</th>\n",
       "      <th>summary</th>\n",
       "      <th>original_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Menko Budi Ungkap Harapan Prabowo untuk 2025: ...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/01/00...</td>\n",
       "      <td>(KOMPAS.com/BAHARUDIN AL FARISI)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-01 00:27:07 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capaian Satgas Ops Damai Cartenz 2024: Duduki ...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/01/17...</td>\n",
       "      <td>(Satgas Damai Cartenz Polri)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-01 17:43:47 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prabowo \"Video Call\" Ribuan Prajurit TNI di Pa...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/01/18...</td>\n",
       "      <td>(Dok. Kementerian Pertahanan)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-01 18:56:01 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sidang Korupsi di Basarnas, Hakim Singgung Ket...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/02/13...</td>\n",
       "      <td>(KOMPAS.com/Syakirun Ni'am)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-02 13:27:06 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Menko Polkam hingga Jaksa Agung Gelar Rakor Pe...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/02/13...</td>\n",
       "      <td>(KOMPAS.COM/ KIKI SAFITRI)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-02 13:48:23 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Menko Budi Ungkap Harapan Prabowo untuk 2025: ...   \n",
       "1  Capaian Satgas Ops Damai Cartenz 2024: Duduki ...   \n",
       "2  Prabowo \"Video Call\" Ribuan Prajurit TNI di Pa...   \n",
       "3  Sidang Korupsi di Basarnas, Hakim Singgung Ket...   \n",
       "4  Menko Polkam hingga Jaksa Agung Gelar Rakor Pe...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://nasional.kompas.com/read/2025/01/01/00...   \n",
       "1  https://nasional.kompas.com/read/2025/01/01/17...   \n",
       "2  https://nasional.kompas.com/read/2025/01/01/18...   \n",
       "3  https://nasional.kompas.com/read/2025/01/02/13...   \n",
       "4  https://nasional.kompas.com/read/2025/01/02/13...   \n",
       "\n",
       "                             author   section          published_local  \\\n",
       "0  (KOMPAS.com/BAHARUDIN AL FARISI)  Nasional  2025-01-01 00:27:07 WIB   \n",
       "1      (Satgas Damai Cartenz Polri)  Nasional  2025-01-01 17:43:47 WIB   \n",
       "2     (Dok. Kementerian Pertahanan)  Nasional  2025-01-01 18:56:01 WIB   \n",
       "3       (KOMPAS.com/Syakirun Ni'am)  Nasional  2025-01-02 13:27:06 WIB   \n",
       "4        (KOMPAS.COM/ KIKI SAFITRI)  Nasional  2025-01-02 13:48:23 WIB   \n",
       "\n",
       "                                             summary  \\\n",
       "0  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "1  JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...   \n",
       "2  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "3  JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...   \n",
       "4  JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...   \n",
       "\n",
       "                                       original_text  \\\n",
       "0  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "1  JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...   \n",
       "2  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "3  JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...   \n",
       "4  JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "1  JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...   \n",
       "2  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "3  JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...   \n",
       "4  JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...   \n",
       "\n",
       "                                cleaned_text_preview  \n",
       "0  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...  \n",
       "1  JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...  \n",
       "2  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...  \n",
       "3  JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...  \n",
       "4  JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre = run_preprocessing(\n",
    "    input_path=r\"D:\\SEMESTER 7\\Equivalensi jurnal P Setio\\coding\\kompas_bencana_nasional_2025.xlsx\",\n",
    "    out_dir=r\"D:\\SEMESTER 7\\Equivalensi jurnal P Setio\\coding\"\n",
    ")\n",
    "\n",
    "df_pre.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af106a6",
   "metadata": {},
   "source": [
    "3. Tahap Ekstraksi event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befe1d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.7.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (78.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (3.2.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install xlsxwriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba894cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Muhammad Nur Iman\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[INFO] Memuat model Sentence-BERT (Multilingual)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7a31f964cb4ba5951521911dc01d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0994d2c9d7b4ac8a6e71add27084d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ae4610c9c34164b6a93c0e8d9503a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02190694866f4c38bbd220d9ad55e90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c187015e5324e01beb30aeb1692d8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da74c4dce4be43eda9229837536134de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faaef6ba10b4c878ec054ee18d07701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4b232def8e41209984b5fabef980fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a1030637484ba1b31b61b14f45df75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c104004ef14b1f8644ab5bda7df246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e4ee6a6f9840c98c52fe0dffb74ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af43626aa1754eb5956ce1f607de1348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3befdd2b70494065922ea3ce63d58c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Mengekstrak event_type...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1429/1429 [00:19<00:00, 74.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUKSES] Hasil disimpan di: event_type_fixed.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>section</th>\n",
       "      <th>published_local</th>\n",
       "      <th>summary</th>\n",
       "      <th>event_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Menko Budi Ungkap Harapan Prabowo untuk 2025: ...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/01/00...</td>\n",
       "      <td>(KOMPAS.com/BAHARUDIN AL FARISI)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-01 00:27:07 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>badai (low confidence)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capaian Satgas Ops Damai Cartenz 2024: Duduki ...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/01/17...</td>\n",
       "      <td>(Satgas Damai Cartenz Polri)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-01 17:43:47 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...</td>\n",
       "      <td>banjir bandang (low confidence)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prabowo \"Video Call\" Ribuan Prajurit TNI di Pa...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/01/18...</td>\n",
       "      <td>(Dok. Kementerian Pertahanan)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-01 18:56:01 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...</td>\n",
       "      <td>banjir bandang (low confidence)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sidang Korupsi di Basarnas, Hakim Singgung Ket...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/02/13...</td>\n",
       "      <td>(KOMPAS.com/Syakirun Ni'am)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-02 13:27:06 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...</td>\n",
       "      <td>banjir bandang (low confidence)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Menko Polkam hingga Jaksa Agung Gelar Rakor Pe...</td>\n",
       "      <td>https://nasional.kompas.com/read/2025/01/02/13...</td>\n",
       "      <td>(KOMPAS.COM/ KIKI SAFITRI)</td>\n",
       "      <td>Nasional</td>\n",
       "      <td>2025-01-02 13:48:23 WIB</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...</td>\n",
       "      <td>banjir bandang (low confidence)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Menko Budi Ungkap Harapan Prabowo untuk 2025: ...   \n",
       "1  Capaian Satgas Ops Damai Cartenz 2024: Duduki ...   \n",
       "2  Prabowo \"Video Call\" Ribuan Prajurit TNI di Pa...   \n",
       "3  Sidang Korupsi di Basarnas, Hakim Singgung Ket...   \n",
       "4  Menko Polkam hingga Jaksa Agung Gelar Rakor Pe...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://nasional.kompas.com/read/2025/01/01/00...   \n",
       "1  https://nasional.kompas.com/read/2025/01/01/17...   \n",
       "2  https://nasional.kompas.com/read/2025/01/01/18...   \n",
       "3  https://nasional.kompas.com/read/2025/01/02/13...   \n",
       "4  https://nasional.kompas.com/read/2025/01/02/13...   \n",
       "\n",
       "                             author   section          published_local  \\\n",
       "0  (KOMPAS.com/BAHARUDIN AL FARISI)  Nasional  2025-01-01 00:27:07 WIB   \n",
       "1      (Satgas Damai Cartenz Polri)  Nasional  2025-01-01 17:43:47 WIB   \n",
       "2     (Dok. Kementerian Pertahanan)  Nasional  2025-01-01 18:56:01 WIB   \n",
       "3       (KOMPAS.com/Syakirun Ni'am)  Nasional  2025-01-02 13:27:06 WIB   \n",
       "4        (KOMPAS.COM/ KIKI SAFITRI)  Nasional  2025-01-02 13:48:23 WIB   \n",
       "\n",
       "                                             summary  \\\n",
       "0  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "1  JAKARTA, KOMPAS.com - Satgas Operasi Damai Car...   \n",
       "2  JAKARTA, KOMPAS.com - Presiden Prabowo Subiant...   \n",
       "3  JAKARTA, KOMPAS.com - Hakim Pengadilan Tipikor...   \n",
       "4  JAKARTA, KOMPAS.com - Jaksa Agung RI ST Burhan...   \n",
       "\n",
       "                        event_type  \n",
       "0           badai (low confidence)  \n",
       "1  banjir bandang (low confidence)  \n",
       "2  banjir bandang (low confidence)  \n",
       "3  banjir bandang (low confidence)  \n",
       "4  banjir bandang (low confidence)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# ====== DAFTAR LABEL JENIS BENCANA ======\n",
    "EVENT_LABELS = [\n",
    "    \"banjir\",\n",
    "    \"banjir bandang\",\n",
    "    \"gempa bumi\",\n",
    "    \"guncangan\",\n",
    "    \"tanah longsor\",\n",
    "    \"pergerakan tanah\",\n",
    "    \"letusan gunung berapi\",\n",
    "    \"kebakaran hutan\",\n",
    "    \"kekeringan\",\n",
    "    \"badai\",\n",
    "    \"topan\",\n",
    "    \"tsunami\",\n",
    "    \"gelombang tinggi\",\n",
    "    \"abrasi\"\n",
    "]\n",
    "\n",
    "# ====== LOAD MODEL SBERT ======\n",
    "def load_classifier():\n",
    "    print(\"[INFO] Memuat model Sentence-BERT (Multilingual)...\")\n",
    "    model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "    return model\n",
    "\n",
    "# ====== KLASIFIKASI EVENT ======\n",
    "def classify_event_type(text, model, threshold=0.20):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"tidak terdeteksi\"\n",
    "\n",
    "    text = text[:500]  # pakai bagian awal saja\n",
    "\n",
    "    emb_text = model.encode(text, convert_to_tensor=True)\n",
    "    emb_labels = model.encode(EVENT_LABELS, convert_to_tensor=True)\n",
    "\n",
    "    scores = util.cos_sim(emb_text, emb_labels)[0]\n",
    "    best_idx = torch.argmax(scores).item()\n",
    "    best_score = scores[best_idx].item()\n",
    "\n",
    "    label = EVENT_LABELS[best_idx]\n",
    "\n",
    "    if best_score < threshold:\n",
    "        return f\"{label} (low confidence)\"\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "# ====== PIPELINE UTAMA ======\n",
    "def run_event_extraction(input_path, summary_col=\"summary\", sheet=None, output=\"event_type_fixed.xlsx\"):\n",
    "\n",
    "    ext = os.path.splitext(input_path)[1].lower()\n",
    "    if ext == \".xlsx\":\n",
    "        df = pd.read_excel(input_path, sheet_name=sheet)\n",
    "    else:\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "    if summary_col not in df.columns:\n",
    "        raise ValueError(f\"Kolom '{summary_col}' tidak ditemukan. Kolom tersedia: {list(df.columns)}\")\n",
    "\n",
    "    summaries = df[summary_col].fillna(\"\").astype(str)\n",
    "\n",
    "    # load SBERT\n",
    "    model = load_classifier()\n",
    "\n",
    "    print(\"[INFO] Mengekstrak event_type...\")\n",
    "    df[\"event_type\"] = [\n",
    "        classify_event_type(text, model) for text in tqdm(summaries)\n",
    "    ]\n",
    "\n",
    "    # simpan hasil\n",
    "    if output.lower().endswith(\".csv\"):\n",
    "        df.to_csv(output, index=False, encoding=\"utf-8\")\n",
    "    else:\n",
    "        df.to_excel(output, index=False, engine=\"xlsxwriter\")\n",
    "\n",
    "    print(f\"\\n[SUKSES] Hasil disimpan di: {output}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==== JALANKAN ====\n",
    "df_event = run_event_extraction(\n",
    "    input_path=\"kompas_bencana_nasional_2025.xlsx\",\n",
    "    summary_col=\"summary\",\n",
    "    sheet=\"Disaster_Only\",\n",
    "    output=\"event_type_fixed.xlsx\"\n",
    ")\n",
    "\n",
    "df_event.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af269eba",
   "metadata": {},
   "source": [
    "Fungsi Prediksi Event Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a72470",
   "metadata": {},
   "source": [
    "4. Tahap FEATURE SELECTION (TF-IDF + FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e68e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (1.15.2)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\muhammad nur iman\\appdata\\roaming\\python\\python312\\site-packages (from smart_open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.4 MB 55.7 kB/s eta 0:07:09\n",
      "    --------------------------------------- 0.5/24.4 MB 55.7 kB/s eta 0:07:09\n",
      "    --------------------------------------- 0.5/24.4 MB 55.7 kB/s eta 0:07:09\n",
      "    --------------------------------------- 0.5/24.4 MB 55.7 kB/s eta 0:07:09\n",
      "    --------------------------------------- 0.5/24.4 MB 55.7 kB/s eta 0:07:09\n",
      "    --------------------------------------- 0.5/24.4 MB 55.7 kB/s eta 0:07:09\n",
      "   - -------------------------------------- 0.8/24.4 MB 86.9 kB/s eta 0:04:32\n",
      "   - -------------------------------------- 0.8/24.4 MB 86.9 kB/s eta 0:04:32\n",
      "   - -------------------------------------- 0.8/24.4 MB 86.9 kB/s eta 0:04:32\n",
      "   - -------------------------------------- 0.8/24.4 MB 86.9 kB/s eta 0:04:32\n",
      "   - -------------------------------------- 0.8/24.4 MB 86.9 kB/s eta 0:04:32\n",
      "   - -------------------------------------- 0.8/24.4 MB 86.9 kB/s eta 0:04:32\n",
      "   - -------------------------------------- 1.0/24.4 MB 108.0 kB/s eta 0:03:37\n",
      "   - -------------------------------------- 1.0/24.4 MB 108.0 kB/s eta 0:03:37\n",
      "   - -------------------------------------- 1.0/24.4 MB 108.0 kB/s eta 0:03:37\n",
      "   - -------------------------------------- 1.0/24.4 MB 108.0 kB/s eta 0:03:37\n",
      "   -- ------------------------------------- 1.3/24.4 MB 128.3 kB/s eta 0:03:00\n",
      "   -- ------------------------------------- 1.3/24.4 MB 128.3 kB/s eta 0:03:00\n",
      "   -- ------------------------------------- 1.3/24.4 MB 128.3 kB/s eta 0:03:00\n",
      "   -- ------------------------------------- 1.6/24.4 MB 150.9 kB/s eta 0:02:32\n",
      "   -- ------------------------------------- 1.6/24.4 MB 150.9 kB/s eta 0:02:32\n",
      "   -- ------------------------------------- 1.6/24.4 MB 150.9 kB/s eta 0:02:32\n",
      "   --- ------------------------------------ 1.8/24.4 MB 166.7 kB/s eta 0:02:16\n",
      "   --- ------------------------------------ 1.8/24.4 MB 166.7 kB/s eta 0:02:16\n",
      "   --- ------------------------------------ 2.1/24.4 MB 186.1 kB/s eta 0:02:00\n",
      "   --- ------------------------------------ 2.1/24.4 MB 186.1 kB/s eta 0:02:00\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   --- ------------------------------------ 2.4/24.4 MB 203.1 kB/s eta 0:01:49\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 179.8 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 179.8 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 179.8 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 179.8 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 179.8 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 179.8 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 179.8 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 179.6 kB/s eta 0:02:00\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 179.6 kB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 191.6 kB/s eta 0:01:51\n",
      "   ----- ---------------------------------- 3.4/24.4 MB 205.6 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 3.7/24.4 MB 219.6 kB/s eta 0:01:35\n",
      "   ------ --------------------------------- 3.9/24.4 MB 232.8 kB/s eta 0:01:28\n",
      "   ------ --------------------------------- 4.2/24.4 MB 246.2 kB/s eta 0:01:23\n",
      "   ------- -------------------------------- 4.5/24.4 MB 259.6 kB/s eta 0:01:17\n",
      "   ------- -------------------------------- 4.7/24.4 MB 272.4 kB/s eta 0:01:13\n",
      "   -------- ------------------------------- 5.0/24.4 MB 284.6 kB/s eta 0:01:09\n",
      "   -------- ------------------------------- 5.0/24.4 MB 284.6 kB/s eta 0:01:09\n",
      "   -------- ------------------------------- 5.2/24.4 MB 293.5 kB/s eta 0:01:06\n",
      "   -------- ------------------------------- 5.2/24.4 MB 293.5 kB/s eta 0:01:06\n",
      "   --------- ------------------------------ 5.5/24.4 MB 300.4 kB/s eta 0:01:03\n",
      "   --------- ------------------------------ 5.5/24.4 MB 300.4 kB/s eta 0:01:03\n",
      "   --------- ------------------------------ 5.5/24.4 MB 300.4 kB/s eta 0:01:03\n",
      "   --------- ------------------------------ 5.5/24.4 MB 300.4 kB/s eta 0:01:03\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 5.8/24.4 MB 300.6 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   --------- ------------------------------ 6.0/24.4 MB 288.4 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 244.5 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 230.4 kB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 230.4 kB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 230.4 kB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 230.4 kB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 230.4 kB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 230.4 kB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 228.3 kB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 228.3 kB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 228.3 kB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 232.5 kB/s eta 0:01:15\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 232.5 kB/s eta 0:01:15\n",
      "   ------------ --------------------------- 7.3/24.4 MB 238.2 kB/s eta 0:01:12\n",
      "   ------------ --------------------------- 7.3/24.4 MB 238.2 kB/s eta 0:01:12\n",
      "   ------------ --------------------------- 7.6/24.4 MB 278.9 kB/s eta 0:01:01\n",
      "   ------------ --------------------------- 7.6/24.4 MB 278.9 kB/s eta 0:01:01\n",
      "   ------------ --------------------------- 7.9/24.4 MB 284.5 kB/s eta 0:00:59\n",
      "   ------------ --------------------------- 7.9/24.4 MB 284.5 kB/s eta 0:00:59\n",
      "   ------------- -------------------------- 8.1/24.4 MB 289.9 kB/s eta 0:00:57\n",
      "   ------------- -------------------------- 8.1/24.4 MB 289.9 kB/s eta 0:00:57\n",
      "   ------------- -------------------------- 8.4/24.4 MB 295.4 kB/s eta 0:00:55\n",
      "   ------------- -------------------------- 8.4/24.4 MB 295.4 kB/s eta 0:00:55\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.7/24.4 MB 301.0 kB/s eta 0:00:53\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   -------------- ------------------------- 8.9/24.4 MB 287.7 kB/s eta 0:00:54\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.2/24.4 MB 274.7 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.4/24.4 MB 256.7 kB/s eta 0:00:59\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   --------------- ------------------------ 9.7/24.4 MB 240.7 kB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 149.1 kB/s eta 0:01:37\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 150.0 kB/s eta 0:01:35\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 138.1 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 113.7 kB/s eta 0:02:01\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 113.7 kB/s eta 0:02:01\n",
      "   ------------------ --------------------- 11.0/24.4 MB 105.5 kB/s eta 0:02:07\n",
      "   ------------------ --------------------- 11.0/24.4 MB 105.5 kB/s eta 0:02:07\n",
      "   ------------------ --------------------- 11.3/24.4 MB 105.8 kB/s eta 0:02:05\n",
      "   ------------------ --------------------- 11.3/24.4 MB 105.8 kB/s eta 0:02:05\n",
      "   ------------------ --------------------- 11.3/24.4 MB 105.8 kB/s eta 0:02:05\n",
      "   ------------------ --------------------- 11.5/24.4 MB 105.2 kB/s eta 0:02:03\n",
      "   ------------------ --------------------- 11.5/24.4 MB 105.2 kB/s eta 0:02:03\n",
      "   ------------------ --------------------- 11.5/24.4 MB 105.2 kB/s eta 0:02:03\n",
      "   ------------------ --------------------- 11.5/24.4 MB 105.2 kB/s eta 0:02:03\n",
      "   ------------------ --------------------- 11.5/24.4 MB 105.2 kB/s eta 0:02:03\n",
      "   ------------------- -------------------- 11.8/24.4 MB 101.1 kB/s eta 0:02:05\n",
      "   ------------------- -------------------- 11.8/24.4 MB 101.1 kB/s eta 0:02:05\n",
      "   ------------------- -------------------- 11.8/24.4 MB 101.1 kB/s eta 0:02:05\n",
      "   ------------------- -------------------- 11.8/24.4 MB 101.1 kB/s eta 0:02:05\n",
      "   ------------------- -------------------- 11.8/24.4 MB 101.1 kB/s eta 0:02:05\n",
      "   ------------------- -------------------- 11.8/24.4 MB 101.1 kB/s eta 0:02:05\n",
      "   ------------------- -------------------- 12.1/24.4 MB 105.8 kB/s eta 0:01:57\n",
      "   ------------------- -------------------- 12.1/24.4 MB 105.8 kB/s eta 0:01:57\n",
      "   ------------------- -------------------- 12.1/24.4 MB 105.8 kB/s eta 0:01:57\n",
      "   -------------------- ------------------- 12.3/24.4 MB 115.6 kB/s eta 0:01:45\n",
      "   -------------------- ------------------- 12.6/24.4 MB 123.6 kB/s eta 0:01:36\n",
      "   -------------------- ------------------- 12.6/24.4 MB 123.6 kB/s eta 0:01:36\n",
      "   --------------------- ------------------ 12.8/24.4 MB 131.4 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 12.8/24.4 MB 131.4 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 12.8/24.4 MB 131.4 kB/s eta 0:01:28\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.1/24.4 MB 138.0 kB/s eta 0:01:22\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   --------------------- ------------------ 13.4/24.4 MB 133.2 kB/s eta 0:01:23\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 140.1 kB/s eta 0:01:17\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 138.6 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 138.6 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 138.6 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 144.1 kB/s eta 0:01:12\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 144.1 kB/s eta 0:01:12\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 144.1 kB/s eta 0:01:12\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 144.1 kB/s eta 0:01:12\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 148.6 kB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 148.6 kB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 148.6 kB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 148.6 kB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 148.6 kB/s eta 0:01:08\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 148.6 kB/s eta 0:01:08\n",
      "   ------------------------ --------------- 14.7/24.4 MB 161.3 kB/s eta 0:01:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 161.3 kB/s eta 0:01:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 161.3 kB/s eta 0:01:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 161.3 kB/s eta 0:01:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 161.3 kB/s eta 0:01:01\n",
      "   ------------------------ --------------- 14.9/24.4 MB 164.4 kB/s eta 0:00:58\n",
      "   ------------------------ --------------- 14.9/24.4 MB 164.4 kB/s eta 0:00:58\n",
      "   ------------------------ --------------- 14.9/24.4 MB 164.4 kB/s eta 0:00:58\n",
      "   ------------------------ --------------- 14.9/24.4 MB 164.4 kB/s eta 0:00:58\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------ --------------- 15.2/24.4 MB 168.8 kB/s eta 0:00:55\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.5/24.4 MB 185.1 kB/s eta 0:00:49\n",
      "   ------------------------- -------------- 15.7/24.4 MB 190.5 kB/s eta 0:00:46\n",
      "   ------------------------- -------------- 15.7/24.4 MB 190.5 kB/s eta 0:00:46\n",
      "   ------------------------- -------------- 15.7/24.4 MB 190.5 kB/s eta 0:00:46\n",
      "   -------------------------- ------------- 16.0/24.4 MB 196.6 kB/s eta 0:00:43\n",
      "   -------------------------- ------------- 16.0/24.4 MB 196.6 kB/s eta 0:00:43\n",
      "   -------------------------- ------------- 16.3/24.4 MB 202.4 kB/s eta 0:00:41\n",
      "   -------------------------- ------------- 16.3/24.4 MB 202.4 kB/s eta 0:00:41\n",
      "   -------------------------- ------------- 16.3/24.4 MB 202.4 kB/s eta 0:00:41\n",
      "   --------------------------- ------------ 16.5/24.4 MB 207.6 kB/s eta 0:00:38\n",
      "   --------------------------- ------------ 16.5/24.4 MB 207.6 kB/s eta 0:00:38\n",
      "   --------------------------- ------------ 16.8/24.4 MB 213.5 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 16.8/24.4 MB 213.5 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 16.8/24.4 MB 213.5 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 17.0/24.4 MB 217.5 kB/s eta 0:00:34\n",
      "   --------------------------- ------------ 17.0/24.4 MB 217.5 kB/s eta 0:00:34\n",
      "   --------------------------- ------------ 17.0/24.4 MB 217.5 kB/s eta 0:00:34\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 222.6 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 222.6 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 17.6/24.4 MB 228.5 kB/s eta 0:00:30\n",
      "   ---------------------------- ----------- 17.6/24.4 MB 228.5 kB/s eta 0:00:30\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 228.9 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 213.6 kB/s eta 0:00:30\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 213.6 kB/s eta 0:00:30\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 213.6 kB/s eta 0:00:30\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 213.6 kB/s eta 0:00:30\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 213.6 kB/s eta 0:00:30\n",
      "   ------------------------------ --------- 18.4/24.4 MB 214.7 kB/s eta 0:00:29\n",
      "   ------------------------------ --------- 18.4/24.4 MB 214.7 kB/s eta 0:00:29\n",
      "   ------------------------------ --------- 18.6/24.4 MB 219.4 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 18.6/24.4 MB 219.4 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 18.6/24.4 MB 219.4 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 18.6/24.4 MB 219.4 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 18.9/24.4 MB 212.1 kB/s eta 0:00:27\n",
      "   ------------------------------- -------- 19.1/24.4 MB 211.9 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 19.1/24.4 MB 211.9 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 19.1/24.4 MB 211.9 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 19.1/24.4 MB 211.9 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 19.1/24.4 MB 211.9 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 19.1/24.4 MB 211.9 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 19.4/24.4 MB 236.3 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 19.4/24.4 MB 236.3 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 19.4/24.4 MB 236.3 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 19.4/24.4 MB 236.3 kB/s eta 0:00:22\n",
      "   -------------------------------- ------- 19.7/24.4 MB 237.6 kB/s eta 0:00:20\n",
      "   -------------------------------- ------- 19.7/24.4 MB 237.6 kB/s eta 0:00:20\n",
      "   -------------------------------- ------- 19.7/24.4 MB 237.6 kB/s eta 0:00:20\n",
      "   -------------------------------- ------- 19.9/24.4 MB 242.6 kB/s eta 0:00:19\n",
      "   -------------------------------- ------- 19.9/24.4 MB 242.6 kB/s eta 0:00:19\n",
      "   -------------------------------- ------- 19.9/24.4 MB 242.6 kB/s eta 0:00:19\n",
      "   --------------------------------- ------ 20.2/24.4 MB 247.1 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 20.2/24.4 MB 247.1 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 20.2/24.4 MB 247.1 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 20.2/24.4 MB 247.1 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 20.2/24.4 MB 247.1 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 20.2/24.4 MB 247.1 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 20.4/24.4 MB 244.5 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 20.4/24.4 MB 244.5 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 20.4/24.4 MB 244.5 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 20.4/24.4 MB 244.5 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 20.4/24.4 MB 244.5 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 20.7/24.4 MB 244.7 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 20.7/24.4 MB 244.7 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 20.7/24.4 MB 244.7 kB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 271.9 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 278.2 kB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 278.2 kB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 278.2 kB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 278.2 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 278.5 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 271.0 kB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 271.0 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 22.0/24.4 MB 287.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 22.0/24.4 MB 287.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 22.3/24.4 MB 293.1 kB/s eta 0:00:08\n",
      "   ------------------------------------ --- 22.5/24.4 MB 299.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 22.8/24.4 MB 305.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 23.1/24.4 MB 312.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 23.3/24.4 MB 319.4 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 23.6/24.4 MB 326.0 kB/s eta 0:00:03\n",
      "   ---------------------------------------  23.9/24.4 MB 331.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  23.9/24.4 MB 331.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  23.9/24.4 MB 331.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  23.9/24.4 MB 331.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  23.9/24.4 MB 331.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  24.1/24.4 MB 329.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 329.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 329.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 329.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 334.3 kB/s eta 0:00:00\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: smart_open, gensim\n",
      "\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   ---------------------------------------- 2/2 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ced447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] kompas_preprocessed.csv\n",
      "[TOKENS] menggunakan kolom: stemmed_tokens\n",
      "[FILTER] 601 dokumen tersisa dari total 1429\n",
      "[TF-IDF] extracting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Nur Iman\\AppData\\Local\\Temp\\ipykernel_15836\\4194198457.py:167: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = df['processed_summary'].str.contains(pattern, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FASTTEXT] training model...\n",
      "\n",
      "== TF-IDF Top 10 ==\n",
      "   word  mean_tfidf\n",
      " banjir    0.046262\n",
      "  gempa    0.038048\n",
      "  bantu    0.034777\n",
      "bencana    0.033437\n",
      " korban    0.032736\n",
      "  hujan    0.029564\n",
      "  bekas    0.027083\n",
      " dampak    0.025475\n",
      "   kita    0.024593\n",
      "myanmar    0.024488\n",
      "\n",
      "== FastText Top 10 ==\n",
      "       word  avg_similarity\n",
      "pascabanjir        0.505284\n",
      "    longsor        0.501656\n",
      "    cempaka        0.482032\n",
      " darussalam        0.478277\n",
      "    gerebek        0.476036\n",
      "     banjir        0.472050\n",
      "      rusuh        0.470048\n",
      "       usir        0.469911\n",
      "     sortir        0.463817\n",
      "   longwave        0.462980\n",
      "\n",
      "[SELESAI] Semua file disimpan ke: results_feature_selection_filtered\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mean_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>banjir</td>\n",
       "      <td>0.046262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gempa</td>\n",
       "      <td>0.038048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bantu</td>\n",
       "      <td>0.034777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bencana</td>\n",
       "      <td>0.033437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>korban</td>\n",
       "      <td>0.032736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  mean_tfidf\n",
       "0   banjir    0.046262\n",
       "1    gempa    0.038048\n",
       "2    bantu    0.034777\n",
       "3  bencana    0.033437\n",
       "4   korban    0.032736"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================================================\n",
    "#   TAHAP 4 — FEATURE SELECTION (TF-IDF + FASTTEXT — FILTERED)\n",
    "# ================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import FastText\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Utility Functions\n",
    "# ------------------------------------------------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def parse_tokens_cell(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(t) for t in x]\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if s.startswith('[') and s.endswith(']'):\n",
    "        try:\n",
    "            lst = ast.literal_eval(s)\n",
    "            if isinstance(lst, list):\n",
    "                return [str(t).strip() for t in lst]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    s = s.replace(\",\", \" \")\n",
    "    return [t for t in s.split() if t]\n",
    "\n",
    "def build_tokens(df):\n",
    "    token_col = None\n",
    "    for cand in ['stemmed_tokens', 'tokens_nostop', 'tokens_summary']:\n",
    "        if cand in df.columns:\n",
    "            token_col = cand\n",
    "            break\n",
    "\n",
    "    if token_col is None:\n",
    "        df['__tokens_tmp'] = df['processed_summary'].astype(str).fillna(\"\").apply(lambda t: t.split())\n",
    "        token_col = '__tokens_tmp'\n",
    "\n",
    "    df[token_col] = df[token_col].apply(parse_tokens_cell)\n",
    "    df['processed_summary'] = df[token_col].apply(lambda toks: \" \".join(toks))\n",
    "\n",
    "    return df, token_col\n",
    "\n",
    "# ------------------------------------------------\n",
    "# TF-IDF\n",
    "# ------------------------------------------------\n",
    "def run_tfidf(texts, max_features=2000, min_df=5, stop_words=None):\n",
    "    vect = TfidfVectorizer(max_features=max_features, min_df=min_df, stop_words=stop_words)\n",
    "    X = vect.fit_transform(texts)\n",
    "    feats = vect.get_feature_names_out()\n",
    "    means = np.asarray(X.mean(axis=0)).ravel()\n",
    "\n",
    "    tfidf_df = pd.DataFrame({'word': feats, 'mean_tfidf': means})\n",
    "    tfidf_df = tfidf_df.sort_values('mean_tfidf', ascending=False).reset_index(drop=True)\n",
    "    return tfidf_df\n",
    "\n",
    "def plot_top_words(df, value_col, topn=30, out_png=None, title=None):\n",
    "    df_top = df.head(topn)\n",
    "    if df_top.empty:\n",
    "        return\n",
    "\n",
    "    words = df_top['word'].tolist()[::-1]\n",
    "    vals = df_top[value_col].tolist()[::-1]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(range(len(words)), vals)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.xlabel(value_col)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# FASTTEXT\n",
    "# ------------------------------------------------\n",
    "def train_fasttext(sentences, vector_size=150, window=5, min_count=1, epochs=20, workers=0):\n",
    "    sentences = [s for s in sentences if isinstance(s, list) and len(s) > 0]\n",
    "    workers = workers if workers > 0 else max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "    model = FastText(vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences, total_examples=len(sentences), epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def most_similar_per_seed(model, seeds, topk=20):\n",
    "    rows = []\n",
    "    for seed in seeds:\n",
    "        if seed in model.wv:\n",
    "            try:\n",
    "                for w, score in model.wv.most_similar(seed, topn=topk):\n",
    "                    rows.append({'seed': seed, 'similar_word': w, 'score': float(score)})\n",
    "            except:\n",
    "                rows.append({'seed': seed, 'similar_word': None, 'score': None})\n",
    "        else:\n",
    "            rows.append({'seed': seed, 'similar_word': None, 'score': None})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def global_similarity(model, seeds, topn=50):\n",
    "    seeds_existing = [s for s in seeds if s in model.wv]\n",
    "    if not seeds_existing:\n",
    "        return pd.DataFrame(columns=['word', 'avg_similarity'])\n",
    "\n",
    "    vocab = list(model.wv.index_to_key)\n",
    "    scores = []\n",
    "    for w in vocab:\n",
    "        vals = [model.wv.similarity(w, s) for s in seeds_existing]\n",
    "        scores.append((w, float(np.mean(vals))))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return pd.DataFrame(scores[:topn], columns=['word','avg_similarity'])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# MAIN PIPELINE\n",
    "# ------------------------------------------------\n",
    "def run_feature_selection_filtered(\n",
    "    input_path,\n",
    "    outdir,\n",
    "    topn=50,\n",
    "    max_features=2000,\n",
    "    min_df=5,\n",
    "    ft_dim=150,\n",
    "    ft_window=5,\n",
    "    ft_min_count=1,\n",
    "    ft_epochs=20,\n",
    "    workers=0,\n",
    "    seeds=\"banjir,gempa,longsor,kebakaran,tsunami\",\n",
    "    topk_per_seed=20,\n",
    "    filter_on=True\n",
    "):\n",
    "    inpath = Path(input_path)\n",
    "    outdir = Path(outdir)\n",
    "    ensure_dir(outdir)\n",
    "    ensure_dir(outdir / \"plots\")\n",
    "\n",
    "    print(f\"[LOAD] {inpath}\")\n",
    "    df = pd.read_csv(inpath)\n",
    "\n",
    "    if 'processed_summary' not in df.columns:\n",
    "        raise ValueError(\"Kolom 'processed_summary' tidak ditemukan.\")\n",
    "\n",
    "    df, token_col = build_tokens(df)\n",
    "    print(f\"[TOKENS] menggunakan kolom: {token_col}\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # FILTERING (opsional)\n",
    "    # --------------------------------------\n",
    "    disaster_keywords = [\"banjir\",\"gempa\",\"longsor\",\"kebakaran\",\"tsunami\",\"erupsi\",\"angin\"]\n",
    "\n",
    "    if filter_on:\n",
    "        pattern = r'\\b(' + '|'.join(disaster_keywords) + r')\\b'\n",
    "        mask = df['processed_summary'].str.contains(pattern, regex=True)\n",
    "        df_filtered = df[mask].copy()\n",
    "        print(f\"[FILTER] {len(df_filtered)} dokumen tersisa dari total {len(df)}\")\n",
    "        if len(df_filtered) == 0:\n",
    "            df_filtered = df.copy()\n",
    "            print(\"[WARNING] Filter kosong → memakai seluruh dataset\")\n",
    "    else:\n",
    "        df_filtered = df.copy()\n",
    "        print(f\"[FILTER OFF] memakai seluruh dokumen: {len(df_filtered)}\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Extra stopwords\n",
    "    # --------------------------------------\n",
    "    extra_stopwords = {\n",
    "        \"jadi\",\"baca\",\"kata\",\"sebut\",\"tidak\",\"ini\",\"itu\",\"akan\",\"yang\",\"dengan\",\n",
    "        \"untuk\",\"pada\",\"oleh\",\"adalah\",\"sudah\",\"tahun\",\"presiden\",\"menteri\",\n",
    "        \"prabowo\",\"tni\",\"indonesia\",\"via\",\"foto\",\"terkait\",\"lainnya\",\"lagi\"\n",
    "    }\n",
    "    stop_words = list(extra_stopwords)\n",
    "\n",
    "    # ---------------- TF-IDF ----------------\n",
    "    print(\"[TF-IDF] extracting...\")\n",
    "    texts = df_filtered['processed_summary'].astype(str)\n",
    "\n",
    "    tfidf_df = run_tfidf(texts, max_features=max_features, min_df=min_df, stop_words=stop_words)\n",
    "    tfidf_df.head(topn).to_csv(outdir / f\"tfidf_top{topn}.csv\", index=False)\n",
    "\n",
    "    plot_top_words(\n",
    "        tfidf_df, 'mean_tfidf', 30,\n",
    "        out_png=outdir / \"plots\" / \"top30_tfidf.png\",\n",
    "        title=\"Top 30 Kata (TF-IDF Filtered)\"\n",
    "    )\n",
    "\n",
    "    # ---------------- FASTTEXT ---------------\n",
    "    print(\"[FASTTEXT] training model...\")\n",
    "    sentences = df_filtered[token_col].tolist()\n",
    "    workers = workers if workers > 0 else max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "    model = train_fasttext(sentences, ft_dim, ft_window, ft_min_count, ft_epochs, workers)\n",
    "    model.save(str(outdir / \"fasttext.model\"))\n",
    "\n",
    "    seeds_list = [s.strip() for s in seeds.split(\",\") if s.strip()]\n",
    "\n",
    "    per_seed_df = most_similar_per_seed(model, seeds_list, topk_per_seed)\n",
    "    per_seed_df.to_csv(outdir / f\"fasttext_top{topk_per_seed}_per_seed.csv\", index=False)\n",
    "\n",
    "    global_df = global_similarity(model, seeds_list, topn)\n",
    "    global_df.to_csv(outdir / f\"fasttext_global_top{topn}.csv\", index=False)\n",
    "\n",
    "    if not global_df.empty:\n",
    "        plot_top_words(\n",
    "            global_df.rename(columns={'avg_similarity': 'mean_tfidf'}),\n",
    "            'mean_tfidf', 30,\n",
    "            out_png=outdir / \"plots\" / \"top30_fasttext.png\",\n",
    "            title=\"Top 30 Kata (FastText Similarity)\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n== TF-IDF Top 10 ==\")\n",
    "    print(tfidf_df.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\n== FastText Top 10 ==\")\n",
    "    print(global_df.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\n[SELESAI] Semua file disimpan ke:\", outdir)\n",
    "\n",
    "    return {\n",
    "        \"tfidf\": tfidf_df,\n",
    "        \"fasttext_global\": global_df,\n",
    "        \"fasttext_per_seed\": per_seed_df,\n",
    "        \"filtered_docs\": df_filtered\n",
    "    }\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "#        JALANKAN PIPELINE\n",
    "# ================================================================\n",
    "results = run_feature_selection_filtered(\n",
    "    input_path=\"kompas_preprocessed.csv\",\n",
    "    outdir=\"results_feature_selection_filtered\",\n",
    "    topn=50,\n",
    "    filter_on=True\n",
    ")\n",
    "\n",
    "results[\"tfidf\"].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d504ce4",
   "metadata": {},
   "source": [
    "5. Tahap (Eksperimen Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "447d01f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading dataset: kompas_bencana_nasional_2025.xlsx\n",
      "Total records: 1429\n",
      "\n",
      "Initial label counts:\n",
      " event_label\n",
      "lainnya                 715\n",
      "banjir                  370\n",
      "gempa                   140\n",
      "kebakaran                85\n",
      "erupsi                   43\n",
      "longsor                  33\n",
      "kekeringan               18\n",
      "tsunami                  14\n",
      "angin_puting_beliung     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Merging rare labels to 'lainnya': []\n",
      "\n",
      "Label distribution after collapse:\n",
      " event_label_collapsed\n",
      "lainnya                 715\n",
      "banjir                  370\n",
      "gempa                   140\n",
      "kebakaran                85\n",
      "erupsi                   43\n",
      "longsor                  33\n",
      "kekeringan               18\n",
      "tsunami                  14\n",
      "angin_puting_beliung     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Training Logistic Regression...\n",
      "\n",
      "LR Best Params: {'clf__C': 10}\n",
      "LR Accuracy: 0.8286713286713286\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "angin_puting_beliung       0.50      0.50      0.50         2\n",
      "              banjir       0.82      0.81      0.82        74\n",
      "              erupsi       1.00      0.56      0.71         9\n",
      "               gempa       0.96      0.82      0.88        28\n",
      "           kebakaran       0.77      0.59      0.67        17\n",
      "          kekeringan       0.50      0.33      0.40         3\n",
      "             lainnya       0.82      0.94      0.87       143\n",
      "             longsor       1.00      0.43      0.60         7\n",
      "             tsunami       0.00      0.00      0.00         3\n",
      "\n",
      "            accuracy                           0.83       286\n",
      "           macro avg       0.71      0.55      0.61       286\n",
      "        weighted avg       0.83      0.83      0.82       286\n",
      "\n",
      "\n",
      " Training Linear SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Nur Iman\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Muhammad Nur Iman\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Muhammad Nur Iman\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Best Params: {'clf__C': 1}\n",
      "SVM Accuracy: 0.8216783216783217\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "angin_puting_beliung       0.50      0.50      0.50         2\n",
      "              banjir       0.85      0.76      0.80        74\n",
      "              erupsi       0.83      0.56      0.67         9\n",
      "               gempa       0.92      0.86      0.89        28\n",
      "           kebakaran       0.71      0.59      0.65        17\n",
      "          kekeringan       0.50      0.33      0.40         3\n",
      "             lainnya       0.81      0.94      0.87       143\n",
      "             longsor       1.00      0.43      0.60         7\n",
      "             tsunami       0.00      0.00      0.00         3\n",
      "\n",
      "            accuracy                           0.82       286\n",
      "           macro avg       0.68      0.55      0.60       286\n",
      "        weighted avg       0.82      0.82      0.81       286\n",
      "\n",
      "\n",
      " Saved models: model_lr_best.pkl, model_svm_best.pkl\n",
      " Saved metrics to: experiment_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Nur Iman\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Muhammad Nur Iman\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Muhammad Nur Iman\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EXPERIMENT FINISHED!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#   Logistic Regression & Linear SVM\n",
    "# =========================================\n",
    "\n",
    "import os, re, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    confusion_matrix, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# CONFIG\n",
    "# ====================================================\n",
    "DATA_PATH = \"kompas_bencana_nasional_2025.xlsx\"\n",
    "RANDOM_STATE = 42\n",
    "MIN_COUNT_THRESHOLD = 10\n",
    "TFIDF_MAX_FEATURES = 5000\n",
    "METRICS_CSV = \"experiment_metrics.csv\"\n",
    "FIG_DIR = \"figures\"\n",
    "\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# PREPROCESSING\n",
    "# ====================================================\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "    text = re.sub(r'[^0-9a-zà-ž\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_excel(path)\n",
    "    df['text_raw'] = (df.get('title', '').fillna(\"\") + \" \" + df.get('summary', '').fillna(\"\")).astype(str)\n",
    "    df['text_clean'] = df['text_raw'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# PSEUDO-LABELING (Heuristik)\n",
    "# ====================================================\n",
    "event_keywords = {\n",
    "    'banjir': ['banjir', 'pascabanjir'],\n",
    "    'gempa': ['gempa','gempabumi','guncang'],\n",
    "    'longsor': ['longsor','tanah longsor'],\n",
    "    'kebakaran': ['kebakaran','bakar'],\n",
    "    'tsunami': ['tsunami'],\n",
    "    'kekeringan': ['kekeringan','kering'],\n",
    "    'angin_puting_beliung': ['puting beliung','angin kencang'],\n",
    "    'erupsi': ['erupsi','letusan','gunung meletus']\n",
    "}\n",
    "\n",
    "def assign_event_label(text):\n",
    "    for label, keys in event_keywords.items():\n",
    "        for k in keys:\n",
    "            if k in text:\n",
    "                return label\n",
    "    return 'lainnya'\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# LOAD DATA\n",
    "# ====================================================\n",
    "print(\" Loading dataset:\", DATA_PATH)\n",
    "df = load_data(DATA_PATH)\n",
    "print(\"Total records:\", len(df))\n",
    "\n",
    "# Pseudo-label\n",
    "df['event_label'] = df['text_clean'].apply(assign_event_label)\n",
    "print(\"\\nInitial label counts:\\n\", df['event_label'].value_counts())\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# MERGE RARE LABELS → 'lainnya'\n",
    "# ====================================================\n",
    "counts = df['event_label'].value_counts()\n",
    "rare = counts[counts < MIN_COUNT_THRESHOLD].index.tolist()\n",
    "\n",
    "print(\"\\nMerging rare labels to 'lainnya':\", rare)\n",
    "\n",
    "df['event_label_collapsed'] = df['event_label'].apply(\n",
    "    lambda x: x if x not in rare else 'lainnya'\n",
    ")\n",
    "\n",
    "print(\"\\nLabel distribution after collapse:\\n\",\n",
    "      df['event_label_collapsed'].value_counts())\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# SPLIT TRAIN-TEST\n",
    "# ====================================================\n",
    "X = df['text_clean'].values\n",
    "y = df['event_label_collapsed'].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "if len(np.unique(y_enc)) < 2:\n",
    "    raise ValueError(\" Error: hanya 1 kelas tersedia setelah pseudo-labeling.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=RANDOM_STATE, stratify=y_enc\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# TF-IDF\n",
    "# ====================================================\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=TFIDF_MAX_FEATURES,\n",
    "                        min_df=2, sublinear_tf=True)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# SKENARIO A — Logistic Regression\n",
    "# ====================================================\n",
    "print(\"\\n Training Logistic Regression...\")\n",
    "pipe_lr = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced',\n",
    "                              random_state=RANDOM_STATE))\n",
    "])\n",
    "param_lr = {'clf__C': [1, 10]}\n",
    "\n",
    "grid_lr = GridSearchCV(pipe_lr, param_lr, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = grid_lr.predict(X_test)\n",
    "\n",
    "print(\"\\nLR Best Params:\", grid_lr.best_params_)\n",
    "print(\"LR Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr, target_names=le.classes_))\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# SKENARIO B — Linear SVM\n",
    "# ====================================================\n",
    "print(\"\\n Training Linear SVM...\")\n",
    "pipe_svm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=TFIDF_MAX_FEATURES,\n",
    "                              min_df=2, sublinear_tf=True)),\n",
    "    ('clf', LinearSVC(class_weight='balanced', max_iter=5000,\n",
    "                      random_state=RANDOM_STATE))\n",
    "])\n",
    "param_svm = {'clf__C': [1, 10]}\n",
    "\n",
    "grid_svm = GridSearchCV(pipe_svm, param_svm, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "grid_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = grid_svm.predict(X_test)\n",
    "\n",
    "print(\"\\nSVM Best Params:\", grid_svm.best_params_)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm, target_names=le.classes_))\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# SAVE MODELS\n",
    "# ====================================================\n",
    "joblib.dump(grid_lr.best_estimator_, \"model_lr_best.pkl\")\n",
    "joblib.dump(grid_svm.best_estimator_, \"model_svm_best.pkl\")\n",
    "print(\"\\n Saved models: model_lr_best.pkl, model_svm_best.pkl\")\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# SAVE METRICS\n",
    "# ====================================================\n",
    "metrics = [\n",
    "    {\n",
    "        'model': 'LogisticRegression',\n",
    "        'accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "        'f1_macro': f1_score(y_test, y_pred_lr, average='macro'),\n",
    "        'best_params': str(grid_lr.best_params_)\n",
    "    },\n",
    "    {\n",
    "        'model': 'LinearSVC',\n",
    "        'accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "        'f1_macro': f1_score(y_test, y_pred_svm, average='macro'),\n",
    "        'best_params': str(grid_svm.best_params_)\n",
    "    }\n",
    "]\n",
    "pd.DataFrame(metrics).to_csv(METRICS_CSV, index=False)\n",
    "print(\" Saved metrics to:\", METRICS_CSV)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# VISUALISATIONS\n",
    "# ====================================================\n",
    "# 1) Label distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(\n",
    "    x=df['event_label_collapsed'].value_counts().index,\n",
    "    y=df['event_label_collapsed'].value_counts().values\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/label_distribution.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 2) Confusion Matrix Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix — Logistic Regression\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/confusion_matrix_lr.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 3) Confusion Matrix SVM\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix — Linear SVM\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIG_DIR}/confusion_matrix_svm.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n EXPERIMENT FINISHED!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
